{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "import time\n",
    "# We used pip install transformers, pip install sentencepiece\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AlbertTokenizer, AlbertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_path = 'projectData/'\n",
    "tb_dir = 'tbs_HW2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: NVIDIA GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "#Define our device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "if torch.cuda.is_available():\n",
    "    print(f'using: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('using cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(data_path+'dataset_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Shuffle the data\n",
    "raw_data = raw_data.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Create dataser class for T5\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data ,label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.label[index]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(input_df, train_size,test_size,batch_size):\n",
    "    train_end_idx = train_size\n",
    "    test_end_idx = train_size + test_size\n",
    "    #Make one list for all the reviews\n",
    "    reviews = input_df['review'].tolist()\n",
    "\n",
    "    #mini sample of reviews for train and test\n",
    "    train_data = reviews[:train_end_idx]\n",
    "    test_data = reviews[train_end_idx:test_end_idx]\n",
    "    #Take mini sample of the labels and preprocess them such that we can use them in the model training loop\n",
    "    labels = input_df['label'].tolist()\n",
    "    train_labels = labels[:train_end_idx]\n",
    "    test_labels = labels[train_end_idx:test_end_idx]\n",
    "\n",
    "    #Use data set class in order to build train and test datasets\n",
    "    train_dataset = SimpleDataset(train_data, train_labels)\n",
    "    test_dataset = SimpleDataset(test_data, test_labels)\n",
    "\n",
    "    #Build train and test dataloaders\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "    return train_dataloader , test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define Tokenizer and 'Bert' Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.decoder.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Define Bert tokenizer and model\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#Define AlBert tokenizer and model\n",
    "bert_tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "bert_model = AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertForSequenceClassification(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bert base uncased architecture\n",
    "bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model: 11685122\n"
     ]
    }
   ],
   "source": [
    "n_parameters = sum(p.numel() for p in bert_model.parameters())\n",
    "print(f\"Number of parameters in model: {n_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizing_batch_Bert(X, y, tokenizer) :\n",
    "    \"\"\"\n",
    "    Does ... TODO\n",
    "    X: Batch of reviews \n",
    "    y: labels of reviews in batch\n",
    "    \"\"\"\n",
    "    inputs =  tokenizer(X, max_length = 128, padding = 'max_length', truncation = True, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    token_type_ids = inputs['token_type_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    labels = torch.LongTensor(y).T.to(device)\n",
    "    \n",
    "    return input_ids, token_type_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training Loop and Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(target,pred):\n",
    "    target, pred = target.to(device),pred.to(device)\n",
    "    correct = torch.sum(target==pred)\n",
    "    accuracy = correct/len(target)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fineTuning(model, optim, epochs, train_dataloader, test_dataloader, tb_dirname, tokenizer,model_type):\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    writer = SummaryWriter(log_dir=f'{tb_dir}/{tb_dirname}_{time.time()}')\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc = \"epochs\", position=0):\n",
    "        running_loss = 0\n",
    "        model.train()\n",
    "        # setup loop with TQDM and dataloader\n",
    "        train_loop = tqdm(train_dataloader, desc='train',position =1, leave=False)\n",
    "        for X, y in train_loop:\n",
    "            # initialize calculated gradients (from prev step)\n",
    "            optim.zero_grad()\n",
    "            # pull all tensor batches required for training\n",
    "            if model_type == 'bert':\n",
    "                input_ids, token_type_ids, attention_mask, labels = tokenizing_batch_Bert(X, y, tokenizer)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels)\n",
    "                \n",
    "\n",
    "            elif model_type == 't5':\n",
    "                ids, mask, labels = tokenizing_batch_T5(X, y, tokenizer)\n",
    "                outputs = model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                labels=labels\n",
    "                )\n",
    "\n",
    "            # process\n",
    "            loss = outputs.loss\n",
    "            # calculate loss for every parameter that needs grad update\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optim.step()\n",
    "            running_loss+=loss.item()\n",
    "            # print relevant info to progress bar\n",
    "            # train_loop.set_description(f'Epoch {epoch}')\n",
    "            train_loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        epoch_train_loss = running_loss/len(train_dataloader.dataset)\n",
    "        train_loss_list.append(epoch_train_loss)\n",
    "\n",
    "\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        model.eval()\n",
    "        # we dont need to update weights, so we define no_grad() to save memory\n",
    "        with torch.no_grad():\n",
    "            eval_loop = tqdm(test_dataloader)\n",
    "            for X_test, y_test in eval_loop:\n",
    "                \n",
    "                if model_type == 'bert':\n",
    "                    input_ids, token_type_ids, attention_mask, labels = tokenizing_batch_Bert(X_test, y_test, tokenizer)\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids,\n",
    "                                labels=labels)\n",
    "                    # get classification\n",
    "                    logits = outputs.logits\n",
    "                    predicted_token_class_ids = logits.argmax(-1)\n",
    "                    # calculate accuracy score\n",
    "                    acc_score = calc_accuracy(y_test, predicted_token_class_ids)\n",
    "                    loss = outputs.loss\n",
    "                \n",
    "                elif model_type == 't5':\n",
    "                    ids, mask, labels = tokenizing_batch_T5(X_test, y_test, tokenizer)\n",
    "                    \n",
    "                    generated_ids = model.generate(\n",
    "                    input_ids=ids,\n",
    "                    attention_mask=mask,\n",
    "                    max_length=2\n",
    "                    )\n",
    "\n",
    "                    # For printing of real words \n",
    "                    preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "                    target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in labels]\n",
    "\n",
    "                    loss = model(input_ids=ids, labels=labels).loss\n",
    "                    acc_score = calc_accuracy(generated_ids[:,1], labels[:,0])\n",
    "                    \n",
    "                test_acc += acc_score.item()\n",
    "                batch_test_loss = outputs.loss.item()\n",
    "                test_loss+= batch_test_loss\n",
    "                eval_loop.set_description(f'Epoch {epoch}')\n",
    "                eval_loop.set_postfix(loss=batch_test_loss)\n",
    "                \n",
    "        epoch_acc_score = test_acc/len(test_dataloader)\n",
    "        test_acc_list.append(epoch_acc_score)\n",
    "        epoch_test_loss = test_loss/len(test_dataloader.dataset)\n",
    "        test_loss_list.append(epoch_test_loss)\n",
    "\n",
    "        writer.add_scalar(tag='loss/train', scalar_value=epoch_train_loss, global_step=epoch)\n",
    "        writer.add_scalar(tag='loss/test', scalar_value=epoch_test_loss, global_step=epoch)\n",
    "       \n",
    "    \n",
    "    return {\n",
    "            'train_loss':train_loss_list,\n",
    "            'test_loss':test_loss_list,\n",
    "            'train_acc':train_acc_list,\n",
    "            'test_acc':test_acc_list\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Enter model to device\n",
    "bert_model = AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\")\n",
    "bert_model.to(device)\n",
    "#Define optimizer\n",
    "optim = torch.optim.AdamW(bert_model.parameters(), lr = 5e-5 )\n",
    "#Define epochs number\n",
    "epochs = 10\n",
    "train_dataloader, test_dataloader = get_data(raw_data,\n",
    "                                            train_size = 500,\n",
    "                                            test_size = 100,\n",
    "                                            batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 32/32 [00:23<00:00,  1.34it/s, loss=0.579]\n",
      "Epoch 0: 100%|██████████| 7/7 [00:01<00:00,  4.61it/s, loss=0.708]\n",
      "Epoch 1: 100%|██████████| 32/32 [00:23<00:00,  1.36it/s, loss=0.575]\n",
      "Epoch 1: 100%|██████████| 7/7 [00:01<00:00,  4.62it/s, loss=0.609]\n",
      "Epoch 2: 100%|██████████| 32/32 [00:23<00:00,  1.36it/s, loss=0.809]\n",
      "Epoch 2: 100%|██████████| 7/7 [00:01<00:00,  4.57it/s, loss=0.49] \n",
      "Epoch 3: 100%|██████████| 32/32 [00:23<00:00,  1.35it/s, loss=0.317]\n",
      "Epoch 3: 100%|██████████| 7/7 [00:01<00:00,  4.49it/s, loss=0.511]\n",
      "Epoch 4: 100%|██████████| 32/32 [00:24<00:00,  1.33it/s, loss=0.178]\n",
      "Epoch 4: 100%|██████████| 7/7 [00:01<00:00,  4.60it/s, loss=0.153]\n",
      "Epoch 5: 100%|██████████| 32/32 [00:23<00:00,  1.36it/s, loss=0.345]\n",
      "Epoch 5: 100%|██████████| 7/7 [00:01<00:00,  4.60it/s, loss=0.115]\n",
      "Epoch 6: 100%|██████████| 32/32 [00:23<00:00,  1.37it/s, loss=0.607] \n",
      "Epoch 6: 100%|██████████| 7/7 [00:01<00:00,  4.53it/s, loss=0.518]\n",
      "Epoch 7: 100%|██████████| 32/32 [00:23<00:00,  1.37it/s, loss=0.0764]\n",
      "Epoch 7: 100%|██████████| 7/7 [00:01<00:00,  4.65it/s, loss=0.0703]\n",
      "Epoch 8: 100%|██████████| 32/32 [00:23<00:00,  1.36it/s, loss=0.0438]\n",
      "Epoch 8: 100%|██████████| 7/7 [00:01<00:00,  4.66it/s, loss=0.555] \n",
      "Epoch 9: 100%|██████████| 32/32 [00:23<00:00,  1.36it/s, loss=0.0205]\n",
      "Epoch 9: 100%|██████████| 7/7 [00:01<00:00,  4.55it/s, loss=0.393] \n"
     ]
    }
   ],
   "source": [
    "#Fine Tuning Bert\n",
    "loss_dic = fineTuning(  model=bert_model,\n",
    "                        optim=optim,\n",
    "                        epochs= epochs,\n",
    "                        train_dataloader = train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        tb_dirname ='Bert',\n",
    "                        tokenizer=bert_tokenizer,\n",
    "                        model_type='bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5803571428571429,\n",
       " 0.6607142857142857,\n",
       " 0.5892857142857143,\n",
       " 0.7232142857142857,\n",
       " 0.6696428571428571,\n",
       " 0.8125,\n",
       " 0.7232142857142857,\n",
       " 0.7589285714285714,\n",
       " 0.7767857142857143,\n",
       " 0.7767857142857143]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_dic['test_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Read the raw data\n",
    "raw_data = pd.read_csv(data_path+'dataset_raw.csv')\n",
    "#Shuffle the data\n",
    "raw_data = raw_data.sample(frac = 1)\n",
    "#Make copy\n",
    "data_T5 = raw_data.copy()\n",
    "#Mapping 0 and 1 to \"positive\" and \"negative\"\n",
    "data_T5['label']=data_T5['label'].map({1: 'positive', 0: 'negative'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Adding the \"sst2 sentence:\" prefix for the reviews\n",
    "def add_T5_preffix_sentimant_classiication(row):\n",
    "    row['review'] = \"sst2 sentence: \" + row['review'] \n",
    "    row['label'] = row['label']\n",
    "    return row\n",
    "\n",
    "data_T5 = data_T5.apply(lambda row: add_T5_preffix_sentimant_classiication(row), axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Make lists for reviews and lables\n",
    "reviews = data_T5['review'].tolist()\n",
    "labels  = data_T5['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Apps\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Define T5 Tokenizer and model\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (relu_act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#T5 small architecture\n",
    "t5_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model: 60506624\n"
     ]
    }
   ],
   "source": [
    "n_parameters = sum(p.numel() for p in t5_model.parameters())\n",
    "print(f\"Number of parameters in model: {n_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizing_batch_T5(X, y, tokenizer) :\n",
    "    train_encoder_inputs = tokenizer(X, padding=\"longest\", max_length=128, truncation=True, return_tensors=\"pt\")\n",
    "    train_decoder_inputs = tokenizer(y, padding=\"longest\", max_length=2, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    labels = train_decoder_inputs[\"input_ids\"]\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    labels = labels.to(device)\n",
    "    ids = train_encoder_inputs[\"input_ids\"].to(device)\n",
    "    mask = train_encoder_inputs[\"input_ids\"].to(device)\n",
    "    \n",
    "    return ids, mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter model to device\n",
    "t5_model.to(device)\n",
    "#Define optimizer\n",
    "optim = torch.optim.AdamW(t5_model.parameters(), lr = 5e-5 )\n",
    "#Define epochs number\n",
    "epochs = 5\n",
    "train_dataloader, test_dataloader = get_data(data_T5,\n",
    "                                            train_size = 750,\n",
    "                                            test_size = 250,\n",
    "                                            batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 16/16 [00:02<00:00,  5.75it/s, loss=7.18]\n",
      "Epoch 1: 100%|██████████| 16/16 [00:02<00:00,  5.49it/s, loss=3.78]\n",
      "Epoch 2: 100%|██████████| 16/16 [00:02<00:00,  5.91it/s, loss=1.21]\n",
      "Epoch 3: 100%|██████████| 16/16 [00:02<00:00,  6.01it/s, loss=0.515]\n",
      "Epoch 4: 100%|██████████| 16/16 [00:02<00:00,  6.15it/s, loss=0.474]\n",
      "epochs: 100%|██████████| 5/5 [01:14<00:00, 14.93s/it]\n"
     ]
    }
   ],
   "source": [
    "loss_dic_t5 = fineTuning(model=t5_model,\n",
    "                        optim=optim,\n",
    "                        epochs= epochs,\n",
    "                        train_dataloader = train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        tb_dirname ='T5',\n",
    "                        tokenizer=t5_tokenizer,\n",
    "                        model_type='t5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.28046875074505806, 0.49765625037252903, 0.48984375037252903]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_dic_t5['test_acc']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d0d24260c26781aab6a6247b3ae992ece4f26fd75ea3713b7a84756a27d5e272"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
